{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This repository is no longer maintained. Please use our new [Softlearning](https://github.com/rail-berkeley/softlearning) package instead.**\n",
    "\n",
    "# Soft Q-Learning\n",
    "Soft Q-learning (SQL) is a deep reinforcement learning framework for training maximum entropy policies in continuous domains. The algorithm is based on the paper [Reinforcement Learning with Deep Energy-Based Policies](https://arxiv.org/abs/1702.08165) presented at the International Conference on Machine Learning (ICML), 2017.\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "Soft Q-learning can be run either locally or through Docker.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You will need to have [Docker](https://docs.docker.com/engine/installation/) and [Docker Compose](https://docs.docker.com/compose/install/) installed unless you want to run the environment locally.\n",
    "\n",
    "Most of the models require a [MuJoCo](https://www.roboti.us/license.html) license.\n",
    "\n",
    "## Docker Installation\n",
    "\n",
    "Currently, rendering of simulations is not supported on Docker due to a missing display setup. As a fix, you can use a [local installation](#local-installation). If you want to run the MuJoCo environments without rendering, the docker environment needs to know where to find your MuJoCo license key (`mjkey.txt`). You can either copy your key into `<PATH_TO_THIS_REPOSITY>/.mujoco/mjkey.txt`, or you can specify the path to the key in your environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export MUJOCO_LICENSE_PATH=<path_to_mujoco>/mjkey.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's done, you can run the Docker container with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker compose creates a Docker container named `soft-q-learning` and automatically sets the needed environment variables and volumes.\n",
    "\n",
    "You can access the container with the typical Docker [exec](https://docs.docker.com/engine/reference/commandline/exec/)-command, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker exec -it soft-q-learning bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See examples section for examples of how to train and simulate the agents.\n",
    "\n",
    "To clean up the setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Installation\n",
    "\n",
    "To get the environment installed correctly, you will first need to clone [rllab](https://github.com/rll/rllab), and have its path added to your PYTHONPATH environment variable.\n",
    "\n",
    "1. Clone rllab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd <installation_path_of_your_choice>\n",
    "git clone https://github.com/rll/rllab.git\n",
    "cd rllab\n",
    "git checkout b3a28992eca103cab3cb58363dd7a4bb07f250a0\n",
    "export PYTHONPATH=$(pwd):${PYTHONPATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [Download](https://www.roboti.us/index.html) and copy MuJoCo files to rllab path:\n",
    "  If you're running on OSX, download https://www.roboti.us/download/mjpro131_osx.zip instead, and copy the `.dylib` files instead of `.so` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p /tmp/mujoco_tmp && cd /tmp/mujoco_tmp\n",
    "wget -P . https://www.roboti.us/download/mjpro131_linux.zip\n",
    "unzip mjpro131_linux.zip\n",
    "mkdir <installation_path_of_your_choice>/rllab/vendor/mujoco\n",
    "cp ./mjpro131/bin/libmujoco131.so <installation_path_of_your_choice>/rllab/vendor/mujoco\n",
    "cp ./mjpro131/bin/libglfw.so.3 <installation_path_of_your_choice>/rllab/vendor/mujoco\n",
    "cd ..\n",
    "rm -rf /tmp/mujoco_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Copy your MuJoCo license key (mjkey.txt) to rllab path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp <mujoco_key_folder>/mjkey.txt <installation_path_of_your_choice>/rllab/vendor/mujoco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Clone `softqlearning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd <installation_path_of_your_choice>\n",
    "git clone https://github.com/haarnoja/softqlearning.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create and activate conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd softqlearning\n",
    "conda env create -f environment.yml\n",
    "source activate sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment should be ready to run. See examples section for examples of how to train and simulate the agents.\n",
    "\n",
    "Finally, to deactivate and remove the conda environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source deactivate\n",
    "conda remove --name sql --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "### Training and simulating an agent\n",
    "1. To train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./examples/mujoco_all_sql.py --env=swimmer --log_dir=\"/root/sql/data/swimmer-experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. To simulate the agent (*NOTE*: This step currently fails with the Docker installation, due to missing display.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./scripts/sim_policy.py /root/sql/data/swimmer-experiment/itr_<iteration>.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mujoco_all_sql.py` contains several different environments and there are more example scripts available in the  `/examples` folder. For more information about the agents and configurations, run the scripts with `--help` flag. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./examples/mujoco_all_sql.py --help\n",
    "usage: mujoco_all_sql.py [-h]\n",
    "                         [--env {ant,walker,swimmer,half-cheetah,humanoid,hopper}]\n",
    "                         [--exp_name EXP_NAME] [--mode MODE]\n",
    "                         [--log_dir LOG_DIR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and combining policies\n",
    "It is also possible to merge two existing maximum entropy policies to form a new composed skill that approximately optimizes both constituent tasks simultaneously as discussed in [ Composable Deep Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/1803.06773). To run the pusher experiment described in the paper, you can first train two policies for the constituent tasks (\"push the object to the given x-coordinate\" and \"push the object to the given y-coordinate\") by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./examples/pusher_pretrain.py --log_dir=/root/sql/data/pusher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then combine the two policies to form a combined skill (\"push the object to the given x and y coordinates\"), without collecting more experience form the environment, with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./examples/pusher_combine.py --log_dir=/root/sql/data/pusher/combined \\\n",
    "--snapshot1=/root/sql/data/pusher/00/params.pkl \\\n",
    "--snapshot2=/root/sql/data/pusher/01/params.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "The soft q-learning algorithm was developed by [Haoran Tang](https://math.berkeley.edu/~hrtang/) and [Tuomas Haarnoja](https://people.eecs.berkeley.edu/~haarnoja/) under the supervision of Prof. [Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/) and Prof. [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/) at UC Berkeley. Special thanks to [Vitchyr Pong](https://github.com/vitchyr), who wrote some parts of the code, and [Kristian Hartikainen](https://github.com/hartikainen) who helped testing, documenting, and polishing the code and streamlining the installation process. The work was supported by [Berkeley Deep Drive](https://deepdrive.berkeley.edu/).\n",
    "\n",
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@article{haarnoja2017reinforcement,\n",
    "  title={Reinforcement Learning with Deep Energy-Based Policies},\n",
    "  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},\n",
    "  booktitle={International Conference on Machine Learning},\n",
    "  year={2017}\n",
    "}\n",
    "@article{haarnoja2018composable,\n",
    "  title={Composable Deep Reinforcement Learning for Robotic Manipulation},\n",
    "  author={Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, Sergey Levine},\n",
    "  booktitle={International Conference on Robotics and Automation},\n",
    "  year={2018}\n",
    "}\n"
   ]
  }
 ],

 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
